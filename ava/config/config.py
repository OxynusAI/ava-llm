class AvaConfig:
    def __init__(
        self,
        vocab_size              = 32000,
        hidden_size             = 2048,
        intermediate_size       = 8192,
        num_hidden_layers       = 16,
        num_attention_heads     = 16,
        hidden_act              = 'silu',
        max_position_embeddings = 2048,
        initializer_range       = 0.02,
        rms_norm_eps            = 1e-5,
        use_cache               = True,
        pad_token_id            = 0,
        bos_token_id            = 1,
        eos_token_id            = 2,
        tie_word_embeddings     = False,
        rope_theta              = 10_000.0,
        attention_dropout       = 0.0,
        model_type              = 'ava',
        **kwargs
    ):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.hidden_act = hidden_act
        self.max_position_embeddings = max_position_embeddings
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.pad_token_id = pad_token_id
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.tie_word_embeddings = tie_word_embeddings
        self.rope_theta = rope_theta
        self.attention_dropout = attention_dropout
        self.model_type = model_type
        self.predefined_models = [
            '100m',
            '500m',
            '1b',
            '3b',
            '7b',
            '13b',
            '30b',
            '65b',
            '100b'
        ]
        
        if 'head_dim' in kwargs:
            self.head_dim = kwargs['head_dim']
        else:
            self.head_dim = hidden_size // num_attention_heads
            
        if 'kv_heads' in kwargs:
            self.kv_heads = kwargs['kv_heads']
        else:
            self.kv_heads = num_attention_heads
            
    def to_dict(self):
        return {k: v for k, v in self.__dict__.items()}


    def apply_for(self, model = '7b'):
        if model not in self.predefined_models:
            raise ValueError(f'Configuration for "{model}" is not defined.')
        
        # Tiny models (Edge devices, IoT, offline agents, chatbots)
        if model == '100m':
            self.hidden_size = 768
            self.intermediate_size = 3072
            self.num_hidden_layers = 6
            self.num_attention_heads = 12
            self.max_position_embeddings = 2048
            self.head_dim = 64
            self.kv_heads = 4

        elif model == '500m':
            self.hidden_size = 1024
            self.intermediate_size = 4096
            self.num_hidden_layers = 8
            self.num_attention_heads = 16
            self.max_position_embeddings = 2048
            self.head_dim = 64
            self.kv_heads = 4

        # Small models (Mobile apps, personal assistants, summarization)
        elif model == '1b':
            self.hidden_size = 1280
            self.intermediate_size = 5120
            self.num_hidden_layers = 12
            self.num_attention_heads = 16
            self.max_position_embeddings = 4096
            self.head_dim = 80
            self.kv_heads = 8

        elif model == '3b':
            self.hidden_size = 1600
            self.intermediate_size = 6400
            self.num_hidden_layers = 24
            self.num_attention_heads = 16
            self.max_position_embeddings = 4096
            self.head_dim = 100
            self.kv_heads = 8

        # Medium models (Coding, reasoning, multi-turn chat, translation)
        elif model == '7b':
            self.hidden_size = 4096
            self.intermediate_size = 11008
            self.num_hidden_layers = 32
            self.num_attention_heads = 32
            self.max_position_embeddings = 8192
            self.head_dim = 128
            self.kv_heads = 8

        elif model == '13b':
            self.hidden_size = 5120
            self.intermediate_size = 13824
            self.num_hidden_layers = 40
            self.num_attention_heads = 40
            self.max_position_embeddings = 8192
            self.head_dim = 128
            self.kv_heads = 8

        # Large models (Research, enterprise-level applications)
        elif model == '30b':
            self.hidden_size = 6656
            self.intermediate_size = 17920
            self.num_hidden_layers = 60
            self.num_attention_heads = 52
            self.max_position_embeddings = 8192
            self.head_dim = 128
            self.kv_heads = 8

        elif model == '65b':
            self.hidden_size = 8192
            self.intermediate_size = 22016
            self.num_hidden_layers = 80
            self.num_attention_heads = 64
            self.max_position_embeddings = 8192
            self.head_dim = 128
            self.kv_heads = 8

        # Massive models (AGI research, cutting-edge LLMs)
        elif model == '100b':
            self.hidden_size = 12288
            self.intermediate_size = 33024
            self.num_hidden_layers = 96
            self.num_attention_heads = 96
            self.max_position_embeddings = 16384
            self.head_dim = 128
            self.kv_heads = 8

        return self
