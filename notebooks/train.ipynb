{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repository Setup and Cleanup\n",
        "This cell performs three important setup operations:\n",
        "\n",
        "1. **Complete Directory Cleanup**:  \n",
        "   `!rm -rf ./* ./.*` - Recursively removes ALL files (including hidden files starting with `.`) in the current directory.  \n",
        "   ‚ö†Ô∏è **Warning**: This is a dangerous command that will permanently delete everything in the current folder.\n",
        "\n",
        "2. **Clone Repository**:  \n",
        "   `!git clone https://github.com/Kuduxaaa/ava-llm .` - Clones the AVA LLM repository from GitHub into the current directory (`.`).\n",
        "\n",
        "3. **Remove Checkpoints**:  \n",
        "   `!rm -rf checkpoints` - Cleans up any existing model checkpoint directories that might have been cloned.\n",
        "\n",
        "**Purpose**: This prepares a clean working environment by:  \n",
        "- Removing any previous files  \n",
        "- Getting the latest code from source  \n",
        "- Ensuring no old checkpoints interfere with new training runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwypXPMfRZ8q"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./* ./.*\n",
        "# !git clone https://github.com/Kuduxaaa/ava-llm .\n",
        "# !rm -rf checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Core Imports and Setup\n",
        "This cell imports all necessary libraries and modules for the AVA language model training pipeline:\n",
        "\n",
        "#### **PyTorch & Core Utilities**\n",
        "- `torch`: Main PyTorch library for deep learning operations\n",
        "- `json`: For handling configuration files and data serialization\n",
        "- `traceback`: For error handling and debugging\n",
        "- `numpy` (`np`): Numerical operations and array handling\n",
        "\n",
        "#### **Hugging Face Components**\n",
        "- `AutoTokenizer`: Tokenizer from Hugging Face's Transformers (likely used as the base tokenizer for AVA)\n",
        "\n",
        "#### **AVA Framework Components**\n",
        "1. **Configuration**:\n",
        "   - `AvaConfig`: Configuration class for the AVA model architecture\n",
        "\n",
        "2. **Model Architecture**:\n",
        "   - `AvaForCausalLM`: The main AVA language model class (causal LM variant)\n",
        "\n",
        "3. **Data Handling**:\n",
        "   - `AvaDataset`: Custom dataset class for AVA training data\n",
        "   - `DataLoader`: PyTorch's data loader for batch processing\n",
        "   - `collate_fn`: Custom collation function for batch preparation\n",
        "\n",
        "4. **Training**:\n",
        "   - `train_model`: Main training loop implementation\n",
        "\n",
        "**Purpose**: This foundational import cell establishes all key components needed for:\n",
        "- Model architecture and configuration\n",
        "- Data loading and preprocessing\n",
        "- The training pipeline execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFQ2ilvyRdHy",
        "outputId": "809e94c3-5d71-446c-84e3-5500aaf74e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ñë‚ñë      ‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë      ‚ñë‚ñë\n",
            "‚ñí  ‚ñí‚ñí‚ñí‚ñí  ‚ñí‚ñí  ‚ñí‚ñí‚ñí‚ñí  ‚ñí‚ñí  ‚ñí‚ñí‚ñí‚ñí  ‚ñí\n",
            "‚ñì  ‚ñì‚ñì‚ñì‚ñì  ‚ñì‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñì‚ñì  ‚ñì‚ñì‚ñì‚ñì  ‚ñì\n",
            "‚ñà        ‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà        ‚ñà\n",
            "‚ñà  ‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà  ‚ñà\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from ava import AvaConfig, AvaForCausalLM\n",
        "from ava.data.datasets import AvaDataset\n",
        "from ava.training.trainer import train_model\n",
        "from ava.utils import collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Model Configuration (500M Parameters)\n",
        "Initializes a 500 million parameter conversational AI model.\n",
        "\n",
        "## üî† Tokenizer Setup\n",
        "**Base Tokenizer:** GPT-2 (Hugging Face)  \n",
        "**Custom Tokens Added:**  \n",
        "- **Structural:** `<|pad|>`, `<|bos|>`, `<|eos|>`  \n",
        "- **Conversational:**  \n",
        "  `<|user|>`, `<|ava|>`  \n",
        "  `<|enduser|>`, `<|endava|>`  \n",
        "\n",
        "## ‚öôÔ∏è Hardware Configuration\n",
        "- **Primary:** CUDA GPU acceleration  \n",
        "- **Fallback:** CPU operation  \n",
        "- Automatic device detection\n",
        "\n",
        "## üîÑ Config-Tokenizer Alignment\n",
        "- Vocabulary size synced  \n",
        "- Special token IDs mapped:  \n",
        "  - `pad_token_id`  \n",
        "  - `bos_token_id`  \n",
        "  - `eos_token_id`  \n",
        "\n",
        "## üö® Memory Considerations\n",
        "**Minimum Requirements:**  \n",
        "- 16GB GPU RAM (training)  \n",
        "- 8GB GPU RAM (inference)  \n",
        "**Recommendations:**  \n",
        "- Start with small batch sizes  \n",
        "- Enable gradient checkpointing  \n",
        "- Consider mixed precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5QsFPFFU_cH",
        "outputId": "154b67e2-0f15-4bc9-b9ac-401fe570db90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "config = AvaConfig().apply_for('500m')\n",
        "tokenizer = AutoTokenizer.from_pretrained('jnz/electra-ka')\n",
        "tokenizer.add_special_tokens({\n",
        "    'pad_token': '<|pad|>',\n",
        "    'bos_token': '<|bos|>',\n",
        "    'eos_token': '<|eos|>',\n",
        "    'unk_token': '<|unk|>',\n",
        "    'cls_token': '<|bos|>',\n",
        "    'sep_token': '<|eos|>',\n",
        "    'additional_special_tokens': [\n",
        "        '<|user|>', \n",
        "        '<|ava|>',\n",
        "        '<|enduser|>',\n",
        "        '<|endava|>'\n",
        "    ]\n",
        "})\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config.vocab_size = len(tokenizer)\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "config.bos_token_id = tokenizer.bos_token_id or tokenizer.eos_token_id\n",
        "config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Data Loading & Preprocessing\n",
        "\n",
        "### Input Data\n",
        "- **Source File:** `/content/data/oasst1_en_conv.json`\n",
        "- **Format:** JSON conversation data\n",
        "- **Encoding:** UTF-8\n",
        "\n",
        "### Processing Steps\n",
        "1. **Initial Load:**\n",
        "   - Entire dataset loaded into memory\n",
        "   - Sample limited to first 50 conversations (`data_small`)\n",
        "\n",
        "2. **Validation Filter:**\n",
        "   - Checks each conversation:\n",
        "     - Must be a list\n",
        "     - Must contain at least 1 message\n",
        "   - Keeps only first message of valid conversations\n",
        "\n",
        "3. **Output:**\n",
        "   - Reports valid/total ratio\n",
        "   - Final valid conversations stored in `valid_data`\n",
        "\n",
        "### Key Variables\n",
        "- `data`: Full dataset (not used after filtering)\n",
        "- `data_small`: First 50 conversations\n",
        "- `valid_data`: Filtered valid conversations\n",
        "\n",
        "> **Note:** This appears to be preparing OASST1 dialogue data for conversational AI training, keeping only the initial messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEVCLWRbVWt8",
        "outputId": "8c648e06-c399-4ef1-ea60-e58780761653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50/50 valid conversations\n"
          ]
        }
      ],
      "source": [
        "with open('/content/data/oasst1_en_conv.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "data_small = data[:50]\n",
        "valid_data = []\n",
        "\n",
        "for conv in data_small:\n",
        "    if isinstance(conv, list) and len(conv) > 0:\n",
        "        valid_data.append(conv[0])\n",
        "\n",
        "print(f'Found {len(valid_data)}/{len(data_small)} valid conversations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÄ Data Splitting & Dataset Preparation\n",
        "\n",
        "### Shuffling & Splitting\n",
        "- Randomly shuffles validated conversations\n",
        "- 90/10 train-validation split\n",
        "  - **Train:** First 90% of shuffled data\n",
        "  - **Validation:** Remaining 10%\n",
        "\n",
        "### Dataset Creation\n",
        "- **Sequence Length:** Fixed at 256 tokens\n",
        "- **Dataset Objects:**\n",
        "  - `train_dataset`: Processed training data\n",
        "  - `val_dataset`: Processed validation data\n",
        "- **Safety Check:** Verifies non-empty datasets\n",
        "\n",
        "### DataLoader Configuration\n",
        "- **Batch Size:** 2 (small for memory efficiency)\n",
        "- **Training Loader:**\n",
        "  - Shuffles batches\n",
        "  - Uses custom `collate_fn`\n",
        "- **Validation Loader:**\n",
        "  - Fixed order\n",
        "  - Same collation function\n",
        "\n",
        "> **Key Parameters:**\n",
        "> - `max_seq_length=256`: Controls token truncation/padding\n",
        "> - `batch_size=2`: Trade-off between memory and gradient stability\n",
        "> - Automatic empty dataset detection prevents silent failures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQbwmM0dVldq",
        "outputId": "0ec40722-c22f-46ff-c838-4c3445562ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 45\n",
            "Validation dataset size: 5\n"
          ]
        }
      ],
      "source": [
        "np.random.shuffle(valid_data)\n",
        "split_idx = int(len(valid_data) * 0.9)\n",
        "train_data = valid_data[:split_idx]\n",
        "val_data = valid_data[split_idx:]\n",
        "\n",
        "max_seq_length = 256\n",
        "train_dataset = AvaDataset(train_data, tokenizer, max_length=max_seq_length)\n",
        "val_dataset = AvaDataset(val_data, tokenizer, max_length=max_seq_length)\n",
        "\n",
        "print(f'Training dataset size: {len(train_dataset)}')\n",
        "print(f'Validation dataset size: {len(val_dataset)}')\n",
        "\n",
        "if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "    raise ValueError('Dataset is empty after processing. Check data format and filtering.')\n",
        "\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = True,\n",
        "    collate_fn = collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    collate_fn = collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Batch Inspection\n",
        "\n",
        "### Purpose\n",
        "Verifies the data loading pipeline by examining:\n",
        "- Tensor shapes\n",
        "- Batch structure\n",
        "- Attention masks\n",
        "- Label formatting\n",
        "\n",
        "### Expected Output\n",
        "- **input_ids:** `[batch_size, sequence_length]`  \n",
        "  (Tokenized input sequences)\n",
        "- **attention_mask:** `[batch_size, sequence_length]`  \n",
        "  (1 for real tokens, 0 for padding)\n",
        "- **labels:** `[batch_size, sequence_length]`  \n",
        "  (Target tokens for language modeling)\n",
        "\n",
        "### Quality Check\n",
        "- Confirms proper batching\n",
        "- Validates tokenizer output\n",
        "- Ensures mask/label alignment\n",
        "- Verifies no shape mismatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Weg-UmVu8a",
        "outputId": "b7ccc43e-1c0a-4fe1-b6f3-0e1700838009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch shapes:\n",
            "input_ids: torch.Size([2, 256])\n",
            "attention_mask: torch.Size([2, 256])\n",
            "labels: torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "print(f'Sample batch shapes:')\n",
        "print(f'input_ids: {sample_batch[\"input_ids\"].shape}')\n",
        "print(f'attention_mask: {sample_batch[\"attention_mask\"].shape}')\n",
        "print(f'labels: {sample_batch[\"labels\"].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Token ID Validation\n",
        "\n",
        "### Purpose\n",
        "Verifies all token IDs are within vocabulary bounds to prevent:\n",
        "- Index errors during training\n",
        "- Invalid token references\n",
        "- Potential model crashes\n",
        "\n",
        "### Checks Performed\n",
        "1. **Finds Maximum Token ID**  \n",
        "   - Scans entire batch for highest ID value\n",
        "2. **Compares Against Vocabulary**  \n",
        "   - Checks tokenizer's vocab size\n",
        "3. **Range Validation**  \n",
        "   - Ensures `max_token_id < vocab_size`\n",
        "\n",
        "### Error Conditions\n",
        "Raises `ValueError` if:\n",
        "- Any token ID exceeds vocabulary size\n",
        "- Tokenizer mapping is misconfigured\n",
        "\n",
        "> **Why This Matters:**  \n",
        "> Catching token ID issues early prevents cryptic failures during forward/backward passes.  \n",
        "> Common causes include:  \n",
        "> - Missing special tokens in vocab  \n",
        "> - Tokenizer/model vocab mismatch  \n",
        "> - Data contamination with invalid tokens  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idEyzG1RVxzX",
        "outputId": "1f5690b0-a80a-42bc-9bd5-8f5029a2f564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum token ID in batch: 50257\n",
            "Tokenizer vocabulary size: 50258\n"
          ]
        }
      ],
      "source": [
        "max_token_id = torch.max(sample_batch['input_ids']).item()\n",
        "print(f'Maximum token ID in batch: {max_token_id}')\n",
        "print(f'Tokenizer vocabulary size: {len(tokenizer)}')\n",
        "\n",
        "if max_token_id >= len(tokenizer):\n",
        "    raise ValueError(f'Maximum token ID {max_token_id} is out of range for vocabulary size {len(tokenizer)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Model & Optimizer Setup\n",
        "\n",
        "### Model Initialization\n",
        "- **Architecture:** `AvaForCausalLM`  \n",
        "  (Custom causal language model)\n",
        "- **Configuration:**  \n",
        "  - 500M parameters  \n",
        "  - Pre-configured token mappings  \n",
        "- **Device Placement:**  \n",
        "  Automatically moves to:  \n",
        "  `GPU (CUDA)` if available  \n",
        "  `CPU` otherwise\n",
        "\n",
        "### Optimizer Configuration\n",
        "- **Type:** AdamW  \n",
        "  (Improved Adam with proper weight decay)\n",
        "- **Key Parameters:**  \n",
        "  - **Learning Rate:** 5e-5  \n",
        "    (Standard for fine-tuning)  \n",
        "  - **Weight Decay:** 0.01  \n",
        "    (Regularization to prevent overfitting)\n",
        "\n",
        "### Critical Checks\n",
        "- All model parameters on correct device\n",
        "- Token embeddings match vocabulary size\n",
        "- Gradient tracking enabled\n",
        "\n",
        "> **Training Ready:**  \n",
        "> This completes the core setup for:  \n",
        "> - Forward/backward passes  \n",
        "> - Gradient updates  \n",
        "> - Parameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7qw4wvIV1Nn"
      },
      "outputs": [],
      "source": [
        "model = AvaForCausalLM(config).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    weight_decay = 0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÇ Training Execution & Safety\n",
        "\n",
        "### Training Process\n",
        "- **Core Training:**  \n",
        "  Runs `train_model` with:  \n",
        "  - 1 epoch (for quick validation)  \n",
        "  - Pre-configured model & data  \n",
        "  - AdamW optimization  \n",
        "\n",
        "### Safety Features\n",
        "1. **Error Handling:**  \n",
        "   - Catches all exceptions  \n",
        "   - Prints detailed traceback  \n",
        "\n",
        "2. **Keyboard Interrupt:**  \n",
        "   - Graceful training cancellation  \n",
        "   - Playful confirmation message  \n",
        "\n",
        "3. **Model Checkpointing:**  \n",
        "   - Saves trained weights to:  \n",
        "   `ava_model_trained.pt`  \n",
        "\n",
        "### Critical Protections\n",
        "- Prevents silent failures  \n",
        "- Preserves partial progress  \n",
        "- Clean exit on interruption  \n",
        "\n",
        "> **Debugging Ready:**  \n",
        "> The verbose error reporting helps diagnose:  \n",
        "> - CUDA memory issues  \n",
        "> - Data loading problems  \n",
        "> - Configuration mismatches  \n",
        "> - Gradient computation errors  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM8CnOtrV3WY",
        "outputId": "0fa35a94-7730-4a38-ff9d-1c74da1598df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ú® Starting training...\n",
            "üçÄ Epoch 1/1 | Batch 0/23 | Loss: 9.4651 | Time: 11.19s\n",
            "üçÄ Epoch 1/1 completed in 176.29s | Average Loss: 8.9878\n",
            "üíæ Checkpoint saved to checkpoints/ava_model_epoch_1.pt\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    train_model(\n",
        "        model        = model,\n",
        "        train_loader = train_loader,\n",
        "        val_loader   = val_loader,\n",
        "        optimizer    = optimizer,\n",
        "        num_epochs   = 1,\n",
        "        device       = device\n",
        "    )\n",
        "\n",
        "    torch.save(model.state_dict(), 'ava_model_trained.pt')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Training error: {e}')\n",
        "    traceback.print_exc()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('üôÑ As you wish, Sir!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ Model Inference & Text Generation\n",
        "\n",
        "### Input Processing\n",
        "- **Prompt Format:**  \n",
        "  `User: What is AI?\\nAssistant:`  \n",
        "  (Uses conversation tokens from tokenizer setup)\n",
        "- **Tokenization:**  \n",
        "  Converts text ‚Üí token IDs ‚Üí PyTorch tensor ‚Üí correct device\n",
        "\n",
        "### Generation Parameters\n",
        "- **Max Length:** 100 tokens  \n",
        "  (Hard cutoff for response length)  \n",
        "- **Temperature:** 0.7  \n",
        "  (Balances creativity vs. predictability)  \n",
        "- **Top-p:** 0.9  \n",
        "  (Nucleus sampling for focused diversity)\n",
        "\n",
        "### Safety Features\n",
        "1. **Full Error Handling:**  \n",
        "   - Catches CUDA/formatting issues  \n",
        "   - Shows complete traceback  \n",
        "2. **Device Awareness:**  \n",
        "   - Automatically uses configured device  \n",
        "3. **Clean Decoding:**  \n",
        "   - Converts tokens ‚Üí human-readable text\n",
        "\n",
        "> **Debug Tip:**  \n",
        "> Adjust temperature (0.3-1.5) and top-p (0.7-0.95) to control:  \n",
        "> - Factual accuracy vs creativity  \n",
        "> - Response variability  \n",
        "> - Hallucination likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZe4r--OV5wJ"
      },
      "outputs": [],
      "source": [
        "input_text = 'User: What is AI?\\nAssistant:'\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "try:\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    print(tokenizer.decode(output[0]))\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Generation error: {e}')\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
